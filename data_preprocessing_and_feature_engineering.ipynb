{"cells":[{"cell_type":"markdown","metadata":{"id":"TyMUMsVUqr0L"},"source":["# Install necessary libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":55767,"status":"ok","timestamp":1681645828146,"user":{"displayName":"Snizhanna Botvyn","userId":"17570915421441911723"},"user_tz":-180},"id":"3ewH3t2BFtOn","outputId":"2d016421-1e00-4373-948d-7b7eff035a80"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tokenize_uk\n","  Downloading tokenize_uk-0.2.0.tar.gz (22 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from tokenize_uk) (1.16.0)\n","Building wheels for collected packages: tokenize_uk\n","  Building wheel for tokenize_uk (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for tokenize_uk: filename=tokenize_uk-0.2.0-py2.py3-none-any.whl size=4588 sha256=4b443c0c317ad85d615e6b300e91e58988ccc1663be2ed6bfa473ade6d4a25cf\n","  Stored in directory: /root/.cache/pip/wheels/df/b5/be/5eba684a792f1b6c4707ba47d29cd55afaac03124b448da2dc\n","Successfully built tokenize_uk\n","Installing collected packages: tokenize_uk\n","Successfully installed tokenize_uk-0.2.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pymorphy3\n","  Downloading pymorphy3-1.2.0-py3-none-any.whl (55 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pymorphy3-dicts-ru\n","  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting docopt>=0.6\n","  Downloading docopt-0.6.2.tar.gz (25 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting dawg-python>=0.7.1\n","  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n","Building wheels for collected packages: docopt\n","  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13721 sha256=f998663419515bdd05c7eddacb18e59b4c9fba1cb7674115c0270deadfad2bda\n","  Stored in directory: /root/.cache/pip/wheels/70/4a/46/1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n","Successfully built docopt\n","Installing collected packages: pymorphy3-dicts-ru, docopt, dawg-python, pymorphy3\n","Successfully installed dawg-python-0.7.2 docopt-0.6.2 pymorphy3-1.2.0 pymorphy3-dicts-ru-2.4.417150.4580142\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting advertools\n","  Downloading advertools-0.13.2-py2.py3-none-any.whl (310 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m310.1/310.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.9/dist-packages (from advertools) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from advertools) (1.5.3)\n","Requirement already satisfied: pyasn1>=0.4 in /usr/local/lib/python3.9/dist-packages (from advertools) (0.4.8)\n","Collecting scrapy>=2.5.0\n","  Downloading Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m272.9/272.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting twython>=3.8.0\n","  Downloading twython-3.9.1-py3-none-any.whl (33 kB)\n","Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->advertools) (1.22.4)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->advertools) (2022.7.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=1.1.0->advertools) (2.8.2)\n","Collecting queuelib>=1.4.2\n","  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n","Collecting pyOpenSSL>=21.0.0\n","  Downloading pyOpenSSL-23.1.1-py3-none-any.whl (57 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting service-identity>=18.1.0\n","  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n","Collecting w3lib>=1.17.0\n","  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n","Collecting Twisted>=18.9.0\n","  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from scrapy>=2.5.0->advertools) (67.6.1)\n","Collecting parsel>=1.5.0\n","  Downloading parsel-1.7.0-py2.py3-none-any.whl (14 kB)\n","Requirement already satisfied: lxml>=4.3.0 in /usr/local/lib/python3.9/dist-packages (from scrapy>=2.5.0->advertools) (4.9.2)\n","Collecting itemadapter>=0.1.0\n","  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n","Collecting protego>=0.1.15\n","  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n","Collecting tldextract\n","  Downloading tldextract-3.4.0-py3-none-any.whl (93 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting PyDispatcher>=2.0.5\n","  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n","Requirement already satisfied: cryptography>=3.4.6 in /usr/local/lib/python3.9/dist-packages (from scrapy>=2.5.0->advertools) (40.0.1)\n","Collecting cssselect>=0.9.1\n","  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n","Collecting zope.interface>=5.1.0\n","  Downloading zope.interface-6.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m246.1/246.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting itemloaders>=1.0.1\n","  Downloading itemloaders-1.0.6-py3-none-any.whl (11 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from scrapy>=2.5.0->advertools) (23.0)\n","Requirement already satisfied: requests-oauthlib>=0.4.0 in /usr/local/lib/python3.9/dist-packages (from twython>=3.8.0->advertools) (1.3.1)\n","Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.9/dist-packages (from twython>=3.8.0->advertools) (2.27.1)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.9/dist-packages (from cryptography>=3.4.6->scrapy>=2.5.0->advertools) (1.15.1)\n","Collecting jmespath>=0.9.5\n","  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from protego>=0.1.15->scrapy>=2.5.0->advertools) (1.16.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (3.4)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.1.0->twython>=3.8.0->advertools) (2.0.12)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.4.0->twython>=3.8.0->advertools) (3.2.2)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.9/dist-packages (from service-identity>=18.1.0->scrapy>=2.5.0->advertools) (0.2.8)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.9/dist-packages (from service-identity>=18.1.0->scrapy>=2.5.0->advertools) (22.2.0)\n","Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.9/dist-packages (from Twisted>=18.9.0->scrapy>=2.5.0->advertools) (4.5.0)\n","Collecting hyperlink>=17.1.1\n","  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting constantly>=15.1\n","  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n","Collecting incremental>=21.3.0\n","  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n","Collecting Automat>=0.8.0\n","  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n","Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.9/dist-packages (from tldextract->scrapy>=2.5.0->advertools) (3.11.0)\n","Collecting requests-file>=1.4\n","  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.9/dist-packages (from cffi>=1.12->cryptography>=3.4.6->scrapy>=2.5.0->advertools) (2.21)\n","Installing collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, queuelib, protego, jmespath, itemadapter, hyperlink, cssselect, Automat, Twisted, requests-file, parsel, twython, tldextract, service-identity, pyOpenSSL, itemloaders, scrapy, advertools\n","Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Twisted-22.10.0 advertools-0.13.2 constantly-15.1.0 cssselect-1.2.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.0.6 jmespath-1.0.1 parsel-1.7.0 protego-0.2.1 pyOpenSSL-23.1.1 queuelib-1.6.2 requests-file-1.5.1 scrapy-2.8.0 service-identity-21.1.0 tldextract-3.4.0 twython-3.9.1 w3lib-2.1.1 zope.interface-6.0\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pymorphy3-dicts-uk\n","  Downloading pymorphy3_dicts_uk-2.4.1.1.1663094765-py2.py3-none-any.whl (8.2 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pymorphy3-dicts-uk\n","Successfully installed pymorphy3-dicts-uk-2.4.1.1.1663094765\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting emot\n","  Downloading emot-3.1-py3-none-any.whl (61 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: emot\n","Successfully installed emot-3.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting stanza\n","  Downloading stanza-1.5.0-py3-none-any.whl (802 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m802.5/802.5 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting emoji\n","  Downloading emoji-2.2.0.tar.gz (240 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m240.9/240.9 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.9/dist-packages (from stanza) (3.20.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from stanza) (1.16.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from stanza) (2.27.1)\n","Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.9/dist-packages (from stanza) (2.0.0+cu118)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from stanza) (4.65.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from stanza) (1.22.4)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (3.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (4.5.0)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (2.0.0)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (3.1.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (1.11.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.3.0->stanza) (3.11.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.3.0->stanza) (16.0.1)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->stanza) (3.4)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.3.0->stanza) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n","Building wheels for collected packages: emoji\n","  Building wheel for emoji (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234926 sha256=ad84e64c76823a4cfd5b9bc5b658ae2b9f40921b951ef7b1ef5833817723efd1\n","  Stored in directory: /root/.cache/pip/wheels/9a/b8/0f/f580817231cbf59f6ade9fd132ff60ada1de9f7dc85521f857\n","Successfully built emoji\n","Installing collected packages: emoji, stanza\n","Successfully installed emoji-2.2.0 stanza-1.5.0\n"]}],"source":["!pip install tokenize_uk\n","!pip install advertools\n","!pip install emot\n","!pip install stanza\n","!pip install emosent-py"]},{"cell_type":"markdown","metadata":{"id":"F6RuitoTq4H5"},"source":["# Download necessary packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"79927127"},"outputs":[],"source":["import pandas as pd\n","import re\n","import advertools as adv\n","import emot as emotic\n","import stanza\n","stanza.download(\"uk\", verbose=False)\n","import nltk\n","import tokenize_uk\n","from nltk.tokenize import TweetTokenizer\n","from emosent import get_emoji_sentiment_rank"]},{"cell_type":"markdown","metadata":{"id":"t9c-mJVBrAQg"},"source":["# Perform text cleaning and part of speech tagging"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9a22a53","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1681645971779,"user_tz":-180,"elapsed":339,"user":{"displayName":"Snizhanna Botvyn","userId":"17570915421441911723"}},"outputId":"690c71df-17bd-4f3f-fad9-00340d10cf7a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["   is_sarcastic                                               text\n","0             1  –≥—Ä–∞—Ñ—ñ–∫–∏ –≤–∏–∂–∏–≤–∞–Ω–Ω—è —Ç—Ä–∏—Ç–æ–Ω—ñ–≤ —ñ –ø—ñ—Ä–∞–º—ñ–¥–∏ —Å–º–µ—Ä—Ç–Ω–æ—Å...\n","1             1  @lovemyself_not –æ–Ω—ñ —Ç–∏ –º–µ–Ω–µ –∑–∞–±–ª–æ–∫—É—î—à —è –ª—é–±–ª—é ...\n","2             1                        @k1207h03 —è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∞ –±–ª—èüòé\n","3             1  –º–µ–Ω–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—å —É —Ä—ñ–∑–Ω—ñ —Å–ø–∏—Å–æ—á–∫–∏ –ø–æ —Ç–∏–ø—É \"–≥...\n","4             1                             @sorixben –º–æ–∂—É —Å—Ç–∞—Ç–∏ üòá"],"text/html":["\n","  <div id=\"df-9645e7ee-0df4-4f80-9611-0a245b915582\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>is_sarcastic</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>–≥—Ä–∞—Ñ—ñ–∫–∏ –≤–∏–∂–∏–≤–∞–Ω–Ω—è —Ç—Ä–∏—Ç–æ–Ω—ñ–≤ —ñ –ø—ñ—Ä–∞–º—ñ–¥–∏ —Å–º–µ—Ä—Ç–Ω–æ—Å...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@lovemyself_not –æ–Ω—ñ —Ç–∏ –º–µ–Ω–µ –∑–∞–±–ª–æ–∫—É—î—à —è –ª—é–±–ª—é ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>@k1207h03 —è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∞ –±–ª—èüòé</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>–º–µ–Ω–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—å —É —Ä—ñ–∑–Ω—ñ —Å–ø–∏—Å–æ—á–∫–∏ –ø–æ —Ç–∏–ø—É \"–≥...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>@sorixben –º–æ–∂—É —Å—Ç–∞—Ç–∏ üòá</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-9645e7ee-0df4-4f80-9611-0a245b915582')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-9645e7ee-0df4-4f80-9611-0a245b915582 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-9645e7ee-0df4-4f80-9611-0a245b915582');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":4}],"source":["# downloading dataset\n","data = pd.read_csv('/content/3795text_original_tag_cleaned.csv', usecols=['NO_TAGS', 'IS_SARCASTIC'])\n","data = data.rename(columns={'NO_TAGS':'text', 'IS_SARCASTIC': 'is_sarcastic'})\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6yeWMarbSn-l"},"outputs":[],"source":["# download stopwords\n","with open('/content/stopwords_ua.txt', encoding='utf-8') as file:\n","    stops = file.read().split()\n","\n","# initialize necessary classes for tokenizarion, emoji extraction and POS tagging  \n","tokenizer = TweetTokenizer()\n","\n","emot_obj = emotic.core.emot()\n","uk_nlp = stanza.Pipeline('uk')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y-7Jy8O9Bg_-"},"outputs":[],"source":["def perform_pos_tagging(text):\n","  \"\"\"Performs part of speech tagging on given text.\n","\n","  Args:\n","    text (str): text to be preprocessed\n","\n","  Returns:\n","    str: part of speech tags separated by spaces\n","  \"\"\"\n","  return ' '.join(word.pos for sent in uk_nlp(text).sentences for word in sent.words)\n","\n","def lemmatize(cleared_text):\n","  return ' '.join(word.lemma for sent in uk_nlp(cleared_text).sentences for word in sent.words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AVF01sSLXgQ5"},"outputs":[],"source":["def normalize_elongated_words(text):\n","    \"\"\"Performs normalization of words with elongated vowels. \n","    For example, '–¥—É—É—É–∂–µ' -> '–¥—É–∂–µ'.\n","\n","  Args:\n","    text (str): text to be preprocessed\n","\n","  Returns:\n","    str: text with normalized elongated words\n","  \"\"\"\n","\n","    regex = re.compile(r'(\\w)\\1+')\n","    normalized = []\n","    for token in tokenizer.tokenize(text):\n","      if regex.search(token):\n","        normalized.append(re.sub(r'(\\w)\\1+', r'\\1', token))\n","      else:\n","        normalized.append(token)\n","    return ' '.join(normalized)\n","\n","def basic_cleaning(text):\n","  \"\"\"Performs basic text cleaning, specifically remove leading/trailing spaces,\n","     latin characters, punctuation, hashtags, links, user mentions, more than\n","     one space, numbers; normalizes elongated word, substitutes apostrophe\n","     with another similar symbol and finally lowercases text.\n","\n","  Args:\n","    text (str): text to be preprocessed\n","\n","  Returns:\n","    str: cleaned text\n","  \"\"\"\n","  text = re.sub('[A-Za-z]+', '', text)\n","  text = re.sub('#\\w+', '', text)\n","  text = re.sub('.pic.\\S+', '', text)\n","  text = re.sub('http\\S+', '', text)\n","  text = re.sub('bit.ly/\\S+', '', text)\n","  text = text.strip('[link]') \n","  text = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', text) \n","  text = text.lower() \n","  text = re.sub('([0-9]+)', '', text)\n","  text = re.sub(r'[\\\\!\"#$%&\\(\\)*+,-./:;<=>?\\[\\]^_`{|}~‚Ä¢@‚Äî‚Äì‚Ä¶¬Ø\\\\„ÉÑ¬Ø‚Äº¬´¬ª‚ò∫Ô∏è‚Äû‚Äú‚Ñ¢]+', '', text)\n","  text = re.sub('\\s+', ' ', text) \n","  text = re.sub('‚Äô', \"'\", text)\n","  text = normalize_elongated_words(text)\n","  text = text.strip()\n","  return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Si3BU0W7LqE5"},"outputs":[],"source":["# clean and POS tag tweets\n","data['cleaned'] = data['text'].apply(lambda x: basic_cleaning(x))\n","data['pos_tags'] = data['cleaned'].apply(lambda x: perform_pos_tagging(x))\n","data['lemmatized'] = data['cleaned'].apply(lambda x: lemmatize(x))\n","# download into separate file\n","data.to_csv('dataset_tagged.csv') "]},{"cell_type":"code","source":["data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":303},"id":"6d6rgRz8rVjm","executionInfo":{"status":"ok","timestamp":1681658834663,"user_tz":-180,"elapsed":23,"user":{"displayName":"Snizhanna Botvyn","userId":"17570915421441911723"}},"outputId":"735c3cc1-f2be-4315-a1b0-f5ad43a25df8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["   is_sarcastic                                               text  \\\n","0             1  –≥—Ä–∞—Ñ—ñ–∫–∏ –≤–∏–∂–∏–≤–∞–Ω–Ω—è —Ç—Ä–∏—Ç–æ–Ω—ñ–≤ —ñ –ø—ñ—Ä–∞–º—ñ–¥–∏ —Å–º–µ—Ä—Ç–Ω–æ—Å...   \n","1             1  @lovemyself_not –æ–Ω—ñ —Ç–∏ –º–µ–Ω–µ –∑–∞–±–ª–æ–∫—É—î—à —è –ª—é–±–ª—é ...   \n","2             1                        @k1207h03 —è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∞ –±–ª—èüòé   \n","3             1  –º–µ–Ω–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—å —É —Ä—ñ–∑–Ω—ñ —Å–ø–∏—Å–æ—á–∫–∏ –ø–æ —Ç–∏–ø—É \"–≥...   \n","4             1                             @sorixben –º–æ–∂—É —Å—Ç–∞—Ç–∏ üòá   \n","\n","                                             cleaned  \\\n","0  –≥—Ä–∞—Ñ—ñ–∫–∏ –≤–∏–∂–∏–≤–∞–Ω—è —Ç—Ä–∏—Ç–æ–Ω—ñ–≤ —ñ –ø—ñ—Ä–∞–º—ñ–¥–∏ —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç...   \n","1  –æ–Ω—ñ —Ç–∏ –º–µ–Ω–µ –∑–∞–±–ª–æ–∫—É—î—à —è –ª—é–±–ª—é –∫–∞—î–±–µ–¥ –≤–æ–Ω–∏ –º—ñ–π ...   \n","2                                 —è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∞ –±–ª—è üòé   \n","3  –º–µ–Ω–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—å —É —Ä—ñ–∑–Ω—ñ —Å–ø–∏—Å–æ—á–∫–∏ –ø–æ —Ç–∏–ø—É –≥—ñ...   \n","4                                       –º–æ–∂—É —Å—Ç–∞—Ç–∏ üòá   \n","\n","                                            pos_tags  \\\n","0    NOUN NOUN NOUN CCONJ NOUN NOUN NOUN ADJ DET ADJ   \n","1  PRON PRON PRON VERB PRON VERB NOUN PRON DET NO...   \n","2                                PRON ADJ NOUN PUNCT   \n","3  PRON PART VERB ADP ADJ NOUN ADP NOUN NOUN PRON...   \n","4                                    VERB VERB PUNCT   \n","\n","                                          lemmatized  \n","0  –≥—Ä–∞—Ñ—ñ–∫ –≤–∏–∂–∏–≤–∞–Ω—è —Ç—Ä–∏—Ç–æ–Ω —ñ –ø—ñ—Ä–∞–º—ñ–¥–∞ —Å–º–µ—Ä—Ç–Ω—ñ—Å—Ç—å –±...  \n","1  –æ–Ω—ñ —Ç–∏ —è –∑–∞–±–ª–æ–∫—É–≤–∞—Ç–∏ —è –ª—é–±–∏—Ç–∏ –∫–∞—î–±–µ–¥ –≤–æ–Ω–∏ –º—ñ–π ...  \n","2                                —è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∏–π –±–ª—è üòé  \n","3  —è –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç–∏ —É —Ä—ñ–∑–Ω–∏–π —Å–ø–∏—Å–æ—á–∫–∞ –ø–æ —Ç–∏–ø –≥—ñ–≤–Ω–æ–∂...  \n","4                                      –º–æ–≥—Ç–∏ —Å—Ç–∞—Ç–∏ üòá  "],"text/html":["\n","  <div id=\"df-8e864862-c6da-4212-83d7-da005eff215c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>is_sarcastic</th>\n","      <th>text</th>\n","      <th>cleaned</th>\n","      <th>pos_tags</th>\n","      <th>lemmatized</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>–≥—Ä–∞—Ñ—ñ–∫–∏ –≤–∏–∂–∏–≤–∞–Ω–Ω—è —Ç—Ä–∏—Ç–æ–Ω—ñ–≤ —ñ –ø—ñ—Ä–∞–º—ñ–¥–∏ —Å–º–µ—Ä—Ç–Ω–æ—Å...</td>\n","      <td>–≥—Ä–∞—Ñ—ñ–∫–∏ –≤–∏–∂–∏–≤–∞–Ω—è —Ç—Ä–∏—Ç–æ–Ω—ñ–≤ —ñ –ø—ñ—Ä–∞–º—ñ–¥–∏ —Å–º–µ—Ä—Ç–Ω–æ—Å—Ç...</td>\n","      <td>NOUN NOUN NOUN CCONJ NOUN NOUN NOUN ADJ DET ADJ</td>\n","      <td>–≥—Ä–∞—Ñ—ñ–∫ –≤–∏–∂–∏–≤–∞–Ω—è —Ç—Ä–∏—Ç–æ–Ω —ñ –ø—ñ—Ä–∞–º—ñ–¥–∞ —Å–º–µ—Ä—Ç–Ω—ñ—Å—Ç—å –±...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>@lovemyself_not –æ–Ω—ñ —Ç–∏ –º–µ–Ω–µ –∑–∞–±–ª–æ–∫—É—î—à —è –ª—é–±–ª—é ...</td>\n","      <td>–æ–Ω—ñ —Ç–∏ –º–µ–Ω–µ –∑–∞–±–ª–æ–∫—É—î—à —è –ª—é–±–ª—é –∫–∞—î–±–µ–¥ –≤–æ–Ω–∏ –º—ñ–π ...</td>\n","      <td>PRON PRON PRON VERB PRON VERB NOUN PRON DET NO...</td>\n","      <td>–æ–Ω—ñ —Ç–∏ —è –∑–∞–±–ª–æ–∫—É–≤–∞—Ç–∏ —è –ª—é–±–∏—Ç–∏ –∫–∞—î–±–µ–¥ –≤–æ–Ω–∏ –º—ñ–π ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>@k1207h03 —è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∞ –±–ª—èüòé</td>\n","      <td>—è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∞ –±–ª—è üòé</td>\n","      <td>PRON ADJ NOUN PUNCT</td>\n","      <td>—è –±—Ä–æ–Ω—å–æ–≤–∞–Ω–∏–π –±–ª—è üòé</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>–º–µ–Ω–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—å —É —Ä—ñ–∑–Ω—ñ —Å–ø–∏—Å–æ—á–∫–∏ –ø–æ —Ç–∏–ø—É \"–≥...</td>\n","      <td>–º–µ–Ω–µ –Ω–µ –¥–æ–±–∞–≤–ª—è—é—Ç—å —É —Ä—ñ–∑–Ω—ñ —Å–ø–∏—Å–æ—á–∫–∏ –ø–æ —Ç–∏–ø—É –≥—ñ...</td>\n","      <td>PRON PART VERB ADP ADJ NOUN ADP NOUN NOUN PRON...</td>\n","      <td>—è –Ω–µ –¥–æ–±–∞–≤–ª—è—Ç–∏ —É —Ä—ñ–∑–Ω–∏–π —Å–ø–∏—Å–æ—á–∫–∞ –ø–æ —Ç–∏–ø –≥—ñ–≤–Ω–æ–∂...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>@sorixben –º–æ–∂—É —Å—Ç–∞—Ç–∏ üòá</td>\n","      <td>–º–æ–∂—É —Å—Ç–∞—Ç–∏ üòá</td>\n","      <td>VERB VERB PUNCT</td>\n","      <td>–º–æ–≥—Ç–∏ —Å—Ç–∞—Ç–∏ üòá</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8e864862-c6da-4212-83d7-da005eff215c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8e864862-c6da-4212-83d7-da005eff215c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8e864862-c6da-4212-83d7-da005eff215c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":46}]},{"cell_type":"markdown","metadata":{"id":"PQFs3BJ8sST8"},"source":["# Perform feature engineering"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lP3FAAMh-kIq"},"outputs":[],"source":["# load data obtained from previous step\n","data = pd.read_csv(\"/content/dataset_tagged.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LqU61-BWouyY"},"outputs":[],"source":["# count number of characters in text\n","def character_count(text):\n","    return len(text)\n","\n","# count number of words in text\n","def words_count(text):\n","    return len([word for word in tokenizer.tokenize(text) if re.search(\"[–ê-–©–¨–Æ–Ø“ê–Ñ–Ü–á–∞-—â—å—é—è“ë—î—ñ—ó'`‚Äô º-]+\", word)])\n","\n","# count ratio of capital characters in text\n","def capital_chars_count(text):\n","    capital_chars = sum(1 for char in text if char.isupper())\n","    if capital_chars:\n","      return capital_chars / character_count(text)\n","    else:\n","      return 0\n","\n","# count ratio of exclamation marks in text\n","def exclamation_mark_count(text):\n","  if text.count('!'):\n","    return text.count(\"!\") / len(tokenizer.tokenize(text))\n","  return 0\n","\n","# count ratio of question marks in text\n","def question_mark_count(text):\n","  if text.count('?'):\n","    return text.count(\"?\") / len(tokenizer.tokenize(text))\n","  return 0\n","\n","# count ratio of elipsis in text\n","def elipsis_count(text):\n","  if text.count('...'):\n","    return text.count(\"...\") / len(tokenizer.tokenize(text))\n","  return 0\n","\n","# count ratio of full stops in text\n","def full_stop_count(text):\n","  if text.count('.'):\n","    return text.count(\".\") / len(tokenizer.tokenize(text))\n","  return 0\n","\n","# count ratio of words in quotes in text\n","def words_in_quotes_count(text, length_in_words):\n","    quoted = re.findall(\"\\'.+\\'|\\\".+\\\"\", text)\n","    if quoted:\n","      return len(quoted) / length_in_words\n","    return 0\n","    \n","# count number of sentences in text\n","def sentences_count(text):\n","    return len((tokenize_uk.tokenize_sents(text)))\n","\n","# count number of unique words in text\n","def unique_words_count(text):\n","    return len(set(tokenizer.tokenize(text)))\n","    \n","# count number of stopwords in text\n","def stopwords_count(text):\n","    return len([w for w in tokenizer.tokenize(text) if w in set(stops)])\n","\n","# count ratio elongated words in text\n","def elongated_words_count(text, length_in_words):\n","    regex = re.compile(r\"([–∞–µ—ñ–æ—É–∏])\\1{2}\")\n","    elongated = sum(1 for word in tokenizer.tokenize(text) if regex.search(word))\n","    if elongated:\n","      return elongated / length_in_words\n","    return 0\n","\n","# count ratio of emoji in text\n","def emojis_count(text, length_in_words):\n","    emoji = len(adv.extract_emoji([text])['emoji'][0])\n","    if emoji:\n","      return emoji / length_in_words\n","    else:\n","      return 0\n","\n","# count ratio of emoticons in text\n","def emoticons_count(text, length_in_words):\n","    emoticons = len(emot_obj.emoticons(text)['value'])\n","    if emoticons:\n","      return emoticons / length_in_words\n","    else:\n","      return 0\n","\n","# count ratio of intensifiers in text\n","def intensifiers_count(pos_tags:str, length_in_words: int):\n","  \"\"\"As intensifiers were considered such sequence of POS tags:\n","      ADV | ADJ + ADV | VERB | ADJ\n","  \"\"\"\n","  counter = 0\n","  pos_tags = pos_tags.split()\n","  for i, tag in enumerate(pos_tags):\n","      if tag in ('ADV', 'ADJ', 'PART'):\n","          try:\n","              if pos_tags[i + 1] in ('PART', 'VERB', 'ADV', 'ADJ'):\n","                  counter += 1\n","          except:\n","              pass\n","  if counter:\n","    return counter / length_in_words\n","  else:\n","    return 0\n","\n","# count ratio of adverbs in text\n","def adverbs_count(pos_tags: str, length_in_words: int):\n","  if pos_tags.count('ADV'):\n","    return pos_tags.count('ADV') / length_in_words\n","  else:\n","    return 0\n","\n","# count ratio of adjectives in text\n","def adjectives_count(pos_tags: str, length_in_words: int):\n","  if pos_tags.count('ADJ'):\n","    return pos_tags.count('ADJ') / length_in_words\n","  else:\n","    return 0\n","\n","# count ratio of interjections in text\n","def interjections_count(pos_tags: str, length_in_words: int):\n","  if pos_tags.count('INTJ'):\n","    return pos_tags.count('INTJ') / length_in_words\n","  else:\n","    return 0\n","\n","# count ratio of particles in text\n","def particles_count(pos_tags: str, length_in_words: int):\n","  if pos_tags.count('PART'):\n","    return pos_tags.count('PART') / length_in_words\n","  else:\n","    return 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x8m0jXic3HGG"},"outputs":[],"source":["# apply functions defined above on text\n","other_features = {'quoted words': [], 'elongated words': [], 'emojis': [],'emoticons': [],\n","                 'intensifiers': [],'adjectives': [],'adverbs': [],'interjections': [], \"particles\": []}\n","\n","for index, row in data.iterrows():\n","  length_in_words = words_count(row['text'])\n","  other_features['quoted words'].append(words_in_quotes_count(row['text'], length_in_words))\n","  other_features['elongated words'].append(elongated_words_count(row['text'], length_in_words))\n","  other_features['emojis'].append(emojis_count(row['text'], length_in_words))\n","  other_features['emoticons'].append(emoticons_count(row['text'], length_in_words))\n","  other_features['intensifiers'].append(intensifiers_count(row['pos_tags'], length_in_words))\n","  other_features['adjectives'].append(adjectives_count(row['pos_tags'], length_in_words))\n","  other_features['adverbs'].append(adverbs_count(row['pos_tags'], length_in_words))\n","  other_features['interjections'].append(interjections_count(row['pos_tags'], length_in_words))\n","  other_features['particles'].append(particles_count(row['pos_tags'], length_in_words))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4558ddb9"},"outputs":[],"source":["# apply functions defined above on text\n","data['characters count'] = data[\"text\"].apply(lambda x:character_count(x))\n","data['words count'] = data[\"text\"].apply(lambda x:words_count(x))\n","data['sentences count'] = data[\"text\"].apply(lambda x:sentences_count(x))\n","data['capital characters count'] = data[\"text\"].apply(lambda x:capital_chars_count(x))\n","data['stopwords count'] = data[\"text\"].apply(lambda x:stopwords_count(x))\n","data['unique words count'] = data[\"text\"].apply(lambda x:unique_words_count(x))\n","data['exclamation marks'] = data[\"text\"].apply(lambda x:exclamation_mark_count(x))\n","data['question marks'] = data[\"text\"].apply(lambda x:question_mark_count(x))\n","data['full stops'] = data[\"text\"].apply(lambda x:full_stop_count(x))\n","data['elipsis'] = data[\"text\"].apply(lambda x:elipsis_count(x))\n","data['average length of word'] = data['characters count']/data['words count']\n","data['average length of sentence'] = data['words count']/data['sentences count']\n","data['ratio of unique words'] = data['unique words count']/data['words count']\n","data['ratio of stop words'] = data['stopwords count']/data['words count']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ixuP3E8ouWTC"},"outputs":[],"source":["# convert one of the features set to dataframe\n","features = pd.DataFrame(other_features)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OYN4lj2oFPuu"},"outputs":[],"source":["# concatenate all features\n","final_features = pd.concat([data, features], axis=1)"]},{"cell_type":"code","source":["final_features.shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LmEqxRLpECqr","executionInfo":{"status":"ok","timestamp":1681658941125,"user_tz":-180,"elapsed":13,"user":{"displayName":"Snizhanna Botvyn","userId":"17570915421441911723"}},"outputId":"efae416f-aca7-4821-8974-ae86b79355b1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7590, 29)"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["final_features.isna().any()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xy4-XaXG9SSU","executionInfo":{"status":"ok","timestamp":1681658943506,"user_tz":-180,"elapsed":15,"user":{"displayName":"Snizhanna Botvyn","userId":"17570915421441911723"}},"outputId":"515b0eea-529d-45e5-b5da-b277b210ba0a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Unnamed: 0                    False\n","is_sarcastic                  False\n","text                          False\n","cleaned                       False\n","pos_tags                      False\n","lemmatized                    False\n","characters count              False\n","words count                   False\n","sentences count               False\n","capital characters count      False\n","stopwords count               False\n","unique words count            False\n","exclamation marks             False\n","question marks                False\n","full stops                    False\n","elipsis                       False\n","average length of word        False\n","average length of sentence    False\n","ratio of unique words         False\n","ratio of stop words           False\n","quoted words                  False\n","elongated words               False\n","emojis                        False\n","emoticons                     False\n","intensifiers                  False\n","adjectives                    False\n","adverbs                       False\n","interjections                 False\n","particles                     False\n","dtype: bool"]},"metadata":{},"execution_count":54}]},{"cell_type":"code","source":["# dowload 2 sentiment dictionaries and combine them\n","\n","words = pd.read_table('/content/tone-dict-uk.tsv')['–í—Å–µ–≤–∏—à–Ω—ñ–π'].to_list()\n","\n","words = [word.lower() for word in words]\n","\n","sent = pd.read_table('/content/tone-dict-uk.tsv')['1'].to_list()\n","\n","sent_words = list(zip(words, sent))\n","\n","\n","def Convert(tup, di):\n","    for a, b in tup:\n","        di.setdefault(a, []).append(b)\n","    return di\n","\n","# Driver Code   \n","tups = sent_words\n","dictionary = {}\n","sent_words = Convert(tups, dictionary)\n","\n","skrup_sent = {}\n","\n","with open(\"sentiment_ua.txt\", \"r\", encoding=\"utf-8\") as f:\n","  for l in f.readlines()[1:]:\n","    skrup_sent[l.split(';')[0].lower()] = l.split(';')[1].strip()\n","\n","def combined_dicts(dic1, dic2):\n","  for key in dic1.keys():\n","    if key not in dic2.keys():\n","      dic2[key] = dic1[key]\n","  return dic2\n","\n","combined = combined_dicts(sent_words, skrup_sent)\n","\n","for k, v in combined.items():\n","  if isinstance(v, list):\n","    combined[k] = v[0]"],"metadata":{"id":"OHgg_n6A3KWL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def sentiment(text):\n","    \"\"\"Calculates sentiment for given text.\n","\n","  Args:\n","    text (str): text to calculate sentiment onn\n","\n","  Returns:\n","    int: sentiment score\n","  \"\"\"\n","    words = text.split()\n","    negative = 0\n","    positive = 0\n","    total = len(words)\n","    for word in words:\n","        if word in combined:\n","            if float(combined[word]) > 0:\n","                positive += 1\n","            else:\n","                negative +=1\n","        else:\n","          try:\n","            if float(get_emoji_sentiment_rank(word)['sentiment_score']) > 0:\n","                positive += 1\n","            elif float(get_emoji_sentiment_rank(word)['sentiment_score']):\n","                negative +=1\n","          except:\n","            continue\n","    pr = positive / total\n","    nr = negative / total\n","    return pr - nr"],"metadata":{"id":"AvOPCFb2gcoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# for key, value in combined.items():\n","#   if int(value) == 2:\n","#     combined[key] = 1\n","#   elif int(value) == -2:\n","#     combined[key] = -1"],"metadata":{"id":"I2bXFYLj4Xm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def find_contradiction(text):\n","    \"\"\"Check whether contradicted sentiments are present in text.\n","\n","  Args:\n","    text (str): text to be preprocessed\n","\n","  Returns:\n","    int: 1 or 0 for True and False for contradiction presence.\n","  \"\"\"\n","  text = text.split()\n","  negative = 0\n","  positive = 0\n","  for word in text:\n","      if word in combined:\n","          if int(combined[word]) > 0:\n","            # print(word)\n","            positive += 1\n","          else:\n","            # print(word)\n","            negative +=1\n","      else:\n","          try:\n","            if float(get_emoji_sentiment_rank(word)['sentiment_score']) > 0:\n","                positive += 1\n","            elif float(get_emoji_sentiment_rank(word)['sentiment_score']):\n","                negative +=1\n","          except:\n","            continue\n","  if positive and negative:\n","    return 1\n","  else:\n","    return 0"],"metadata":{"id":"OD8kD1Ud5ize"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['sentiment'] = df['lemmatized'].apply(sentiment)\n","df['contradiction'] = df['lemmatized'].apply(find_contradiction)"],"metadata":{"id":"8G9eFLfv8uza"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.to_csv('sent_contra_added.csv')"],"metadata":{"id":"4IXv24Hn85DI"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOXmVKQ8APxdgJYh/LzMHda"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}